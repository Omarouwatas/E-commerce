import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
from jobs.utils import extract_sequences_from_mongo

def train_lstm():
    print("üöÄ Lancement du job de r√©entra√Ænement LSTM...")

    # √âtape 1 : (r√©)extraire les donn√©es de Mongo + g√©n√©rer X/y
    X, y = extract_sequences_from_mongo(window_size=3)

    if len(X) < 10:
        print("‚ö†Ô∏è Pas assez de donn√©es pour entra√Æner.")
        return

    # √âtape 2 : charger ancien mod√®le ou en cr√©er un nouveau
    try:
        model = load_model("models/lstm_model.h5")
        print("‚úÖ Mod√®le existant charg√©.")
    except:
        print("üì¶ Aucun mod√®le trouv√©. Cr√©ation d‚Äôun nouveau.")
        model = Sequential()
        model.add(LSTM(64, activation='tanh', input_shape=(X.shape[1], X.shape[2]), return_sequences=False))
        model.add(Dropout(0.2))
        model.add(Dense(32, activation='relu'))
        model.add(Dense(1))
        model.compile(optimizer='adam', loss='mse', metrics=['mae'])

    # √âtape 3 : entra√Ænement incr√©mental
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
    model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=50,
        batch_size=16,
        callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],
        verbose=1
    )

    # √âtape 4 : sauvegarde
    model.save("models/lstm_model.h5")
    print("‚úÖ Mod√®le LSTM r√©entra√Æn√© et sauvegard√©.")
